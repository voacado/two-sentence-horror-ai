{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Core Imports\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Model-related Imports\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration # fine-tuned BART model\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification # restore punct\n",
    "from transformers import pipeline # restore punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/avocado/Developer/Projects/two-sentence-horror-lm/two-sentence-horror-lm/models/bart\n"
     ]
    }
   ],
   "source": [
    "# Double-check current working directory\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model to restore punctuation\n",
    "\n",
    "punct_model_path = \"felflare/bert-restore-punctuation\"\n",
    "\n",
    "punct_tokenizer = AutoTokenizer.from_pretrained(punct_model_path)\n",
    "punct_model = AutoModelForTokenClassification.from_pretrained(punct_model_path)\n",
    "\n",
    "punct_restorer = pipeline(\"token-classification\", model=punct_model, tokenizer=punct_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2b3f6c4dbd4a83ba288ac320b86609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2287b1b3016249599e22bb4f9112bc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28525c3bbb64551b7cf529735dcc290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94448d307e604c0a9ae4e99ee54fdf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f5dd8bf8114966abb0c751f9a7b2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259bc95dc5b24d1584bbf6b2b3867eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085e7eeb7a2c41c9b7030a4da8a93203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to model dir\n",
    "model_path = 'voacado/bart-two-sentence-horror'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "# Load model\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If GPU, use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_punctuation(text, restorer):\n",
    "    # Use the model to predict punctuation\n",
    "    punctuated_output = restorer(text)\n",
    "    punctuated_text = []\n",
    "    \n",
    "    # Define punctuation marks (note: not including left-side because we want space still)\n",
    "    punctuation_marks = [\"!\", \"?\", \".\", \"-\", \":\", \";\", \"'\", \"’\", \",\", \")\", \"]\", \"}\", \"…\", \"”\", \"’’\", \"''\"]\n",
    "    \n",
    "    for elem in punctuated_output:\n",
    "        cur_token = elem.get('word')\n",
    "        \n",
    "        # If token is punctuation, append to previous token\n",
    "        if cur_token in punctuation_marks:\n",
    "            punctuated_text[-1] += cur_token\n",
    "            \n",
    "        # If previous token is quotations, append to previous token\n",
    "        elif punctuated_text and punctuated_text[-1] in [\"'\", \"’\", \"“\", \"‘\", \"‘‘\", \"““\"]:\n",
    "            punctuated_text[-1] += cur_token\n",
    "            \n",
    "        # If token is a contraction or a quote, append to previous token (no space)\n",
    "        elif cur_token.lower() in [\"s\", \"t\", \"re\", \"ve\", \"ll\", \"d\", \"m\"]:\n",
    "            # Remove space for contractions\n",
    "            punctuated_text[-1] += cur_token\n",
    "            \n",
    "        # if prediction is LABEL_0, token should be capitalized\n",
    "        elif elem.get('entity') == 'LABEL_0':\n",
    "            punctuated_text.append(cur_token.capitalize())\n",
    "\n",
    "        # else if prediction is LABEL_1, token should be lowercase\n",
    "        # elif elem.get('entity') == 'LABEL_1':\n",
    "        else:\n",
    "            punctuated_text.append(cur_token)\n",
    "            \n",
    "    # If there's no period at the end of the story, add one\n",
    "    if punctuated_text[-1][-1] != '.':\n",
    "        punctuated_text[-1] = punctuated_text[-1] + '.'\n",
    "\n",
    "    return ' '.join(punctuated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, model, tokenizer, max_length=50):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_length=max_length)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Restore punctuation\n",
    "    generated_text_punct = restore_punctuation(generated_text, punct_restorer)\n",
    "    \n",
    "    return generated_text_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I heard a noise from the bathroom. It was only when I turned on the lights that I realized the noise was coming from the bathroom.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_sentence = \"I heard a noise from the bathroom.\"\n",
    "generated_sentence = generate_text(input_sentence, model, tokenizer)\n",
    "print(input_sentence + ' ' + generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "input_sentences = ['I got out of bed this morning.',\n",
    "                   'I was horrified when I got my test results back.',\n",
    "                   'My parents told me not to go upstairs.',\n",
    "                   'There was a knock on the door.',\n",
    "                   'I was walking home from school.',\n",
    "                   'My friend told me to go to the bathroom.',\n",
    "                   'There was a loud noise coming from the basement.',\n",
    "                   'There was a ghost.',\n",
    "                   'I heard someone whispering in my ear.'\n",
    "]\n",
    "generated_stories = []\n",
    "\n",
    "for input_sentence in input_sentences:\n",
    "    generated_text = generate_text(input_sentence, model, tokenizer)\n",
    "    generated_stories.append(input_sentence + ' ' + generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got out of bed this morning. When I woke up, I saw my reflection in the mirror.\n",
      "\n",
      "\n",
      "I was horrified when I got my test results back. It was only when I got home that I realized they weren’t human.\n",
      "\n",
      "\n",
      "My parents told me not to go upstairs. I don’t know what’s worse, the fact that I’m the only one down here, or that I can hear them screaming.\n",
      "\n",
      "\n",
      "There was a knock on the door. It was the only way I could get out of the basement.\n",
      "\n",
      "\n",
      "I was walking home from school. But when I turned around, I saw a man with a knife in his hand.\n",
      "\n",
      "\n",
      "My friend told me to go to the bathroom. I didn’t expect him to come back.\n",
      "\n",
      "\n",
      "There was a loud noise coming from the basement. It was only when I turned on the lights that I realized the noise wasn't coming from the basement.\n",
      "\n",
      "\n",
      "There was a ghost. It was the only thing keeping me alive.\n",
      "\n",
      "\n",
      "I heard someone whispering in my ear. I thought it was just a hall ##uc ##ination, until I heard a voice whisper back, “don’t worry, you’re not alone.”.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for generated_story in generated_stories:\n",
    "    print(generated_story)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

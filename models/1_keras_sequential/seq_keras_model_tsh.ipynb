{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7114728,"sourceType":"datasetVersion","datasetId":3916095}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Keras Imports (Data prep, model training)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, TimeDistributed, Dropout\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n\n\n# To Create and Load Word Embeddings\nfrom gensim.models import Word2Vec, KeyedVectors\n\n\n# For Tokenizer\nfrom nltk.tokenize import word_tokenize\nimport json\nimport io\n\nimport pandas as pd\nimport numpy as np\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:03:43.445163Z","iopub.execute_input":"2023-12-04T20:03:43.445903Z","iopub.status.idle":"2023-12-04T20:04:01.515445Z","shell.execute_reply.started":"2023-12-04T20:03:43.445854Z","shell.execute_reply":"2023-12-04T20:04:01.514676Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Variables\n\n# Data\n# file_path = '/Users/avocado/Developer/Projects/two-sentence-horror-lm/two-sentence-horror-keras-sequential/dataset/reddit_scrape_20_cleansed.csv'\nfile_path = '/kaggle/input/two-sentence-horror-jan-2015-apr-2023/reddit_cleansed_data.csv'\n\nNGRAM = 3\nEMBEDDINGS_SIZE = 100","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:01.517122Z","iopub.execute_input":"2023-12-04T20:04:01.517630Z","iopub.status.idle":"2023-12-04T20:04:01.522309Z","shell.execute_reply.started":"2023-12-04T20:04:01.517603Z","shell.execute_reply":"2023-12-04T20:04:01.521184Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 1. Load Data","metadata":{}},{"cell_type":"code","source":"# Read in data into Pandas DF\n\ndf = pd.read_csv(file_path)\n\n# Remove NaN values\ndf = df.dropna()\n\n# Remove rows containing chars other than ASCII chars\n# df = df[df['title'].str.contains(r'[^\\x00-\\x7F]+') == False]\n\n# Create a boolean mask for rows to keep\n# Text contains \"&amp;#x200B;\" which is a zero-width space (tokenized incorrectly)\nmask = ~(\n    df['title'].str.contains('x200B|&amp;#x200B;', na=False) |\n    df['selftext'].str.contains('x200B|&amp;#x200B;', na=False)\n)\n\n# Apply the mask to filter the DataFrame\ndf = df[mask]\n\n# Convert 'title' and 'selftext' to lowercase string (lowercase to match case in Word2Vec embedding and Keras Tokenizer)\ndf['title'] = pd.Series(df['title'], dtype=\"string\").str.lower()\ndf['selftext'] = pd.Series(df['selftext'], dtype=\"string\").str.lower()\n\n# df.dtypes\ndf","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:01.523493Z","iopub.execute_input":"2023-12-04T20:04:01.523765Z","iopub.status.idle":"2023-12-04T20:04:02.635595Z","shell.execute_reply.started":"2023-12-04T20:04:01.523721Z","shell.execute_reply":"2023-12-04T20:04:02.634581Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                   title  \\\n0      \"do not expose any part of your body to the air.\"   \n1      i sometimes remember the way he looked, broken...   \n2      i live alone on the third floor of my apartmen...   \n3      i heard the rain hitting my window, so i walke...   \n4      you know how sometimes your brain plays tricks...   \n...                                                  ...   \n94081  as i look thru at window i see something inhumane   \n94082  i’ve always been passionate about conspiracy t...   \n94083  \"you'll see me on the red carpet one day,\" sai...   \n94084  i could hear my sister screaming nearby as i s...   \n94085  when my neighbor’s girlfriend asked for my hel...   \n\n                                                selftext  score  num_comments  \\\n0                      \"i repeat..this is not a drill..\"   65.0           5.0   \n1                  i neglected to make sure he was dead.   22.0           0.0   \n2      so who opens my window every night while i'm s...   35.0           3.0   \n3      my window wasn't wet, but the glass was covere...   28.0           3.0   \n4                    i caught one of those things today.   84.0           6.0   \n...                                                  ...    ...           ...   \n94081  my reflection helps me remember how well my su...   31.0           2.0   \n94082  so when my wife had twins, i knew exactly what...   27.0           8.0   \n94083  so i paid her a surprise visit, and upon walki...   23.0           2.0   \n94084  but my heart sank when i remembered the monste...   60.0           3.0   \n94085  later when the police found parts of my neighb...   33.0           2.0   \n\n       gilded_count          date            timestamp  \n0                 0  1.428090e+09  2015-04-03 19:47:13  \n1                 0  1.428235e+09  2015-04-05 11:55:10  \n2                 0  1.428370e+09  2015-04-07 01:24:42  \n3                 0  1.428385e+09  2015-04-07 05:40:55  \n4                 0  1.428563e+09  2015-04-09 07:03:16  \n...             ...           ...                  ...  \n94081             0  1.680377e+09  2023-04-01 19:21:54  \n94082             0  1.680377e+09  2023-04-01 19:24:55  \n94083             0  1.680378e+09  2023-04-01 19:38:03  \n94084             0  1.680378e+09  2023-04-01 19:41:01  \n94085             0  1.680379e+09  2023-04-01 19:54:23  \n\n[94086 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>selftext</th>\n      <th>score</th>\n      <th>num_comments</th>\n      <th>gilded_count</th>\n      <th>date</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"do not expose any part of your body to the air.\"</td>\n      <td>\"i repeat..this is not a drill..\"</td>\n      <td>65.0</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>1.428090e+09</td>\n      <td>2015-04-03 19:47:13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i sometimes remember the way he looked, broken...</td>\n      <td>i neglected to make sure he was dead.</td>\n      <td>22.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1.428235e+09</td>\n      <td>2015-04-05 11:55:10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i live alone on the third floor of my apartmen...</td>\n      <td>so who opens my window every night while i'm s...</td>\n      <td>35.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>1.428370e+09</td>\n      <td>2015-04-07 01:24:42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i heard the rain hitting my window, so i walke...</td>\n      <td>my window wasn't wet, but the glass was covere...</td>\n      <td>28.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>1.428385e+09</td>\n      <td>2015-04-07 05:40:55</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>you know how sometimes your brain plays tricks...</td>\n      <td>i caught one of those things today.</td>\n      <td>84.0</td>\n      <td>6.0</td>\n      <td>0</td>\n      <td>1.428563e+09</td>\n      <td>2015-04-09 07:03:16</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>94081</th>\n      <td>as i look thru at window i see something inhumane</td>\n      <td>my reflection helps me remember how well my su...</td>\n      <td>31.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.680377e+09</td>\n      <td>2023-04-01 19:21:54</td>\n    </tr>\n    <tr>\n      <th>94082</th>\n      <td>i’ve always been passionate about conspiracy t...</td>\n      <td>so when my wife had twins, i knew exactly what...</td>\n      <td>27.0</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>1.680377e+09</td>\n      <td>2023-04-01 19:24:55</td>\n    </tr>\n    <tr>\n      <th>94083</th>\n      <td>\"you'll see me on the red carpet one day,\" sai...</td>\n      <td>so i paid her a surprise visit, and upon walki...</td>\n      <td>23.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.680378e+09</td>\n      <td>2023-04-01 19:38:03</td>\n    </tr>\n    <tr>\n      <th>94084</th>\n      <td>i could hear my sister screaming nearby as i s...</td>\n      <td>but my heart sank when i remembered the monste...</td>\n      <td>60.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>1.680378e+09</td>\n      <td>2023-04-01 19:41:01</td>\n    </tr>\n    <tr>\n      <th>94085</th>\n      <td>when my neighbor’s girlfriend asked for my hel...</td>\n      <td>later when the police found parts of my neighb...</td>\n      <td>33.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.680379e+09</td>\n      <td>2023-04-01 19:54:23</td>\n    </tr>\n  </tbody>\n</table>\n<p>94086 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2. Encode text into integers (tokenization)","metadata":{}},{"cell_type":"code","source":"# Tokenize data - Functions\n\ndef tokenize_and_add_tokens(sentence: str, ngram_value: int) -> str:\n    \"\"\"\n    Tokenize sentence based on n-gram value (appends/prepends <s> and </s> tokens)\n\n    Args:\n        sentence (str): sentence to tokenize\n        ngram_value (int): number of n-grams to use (size of window)\n\n    Returns:\n        (str): output sentence, tokenized\n    \"\"\"\n    if not isinstance(sentence, str):\n        print(\"ERROR:\", sentence)\n    \n    # Tokenize the sentence\n    tokens = word_tokenize(sentence)\n\n    # Add the <s> and </s> tokens based on n-gram value\n    start_tokens = ['<s>'] * (ngram_value - 1)\n    end_tokens = ['</s>'] * (ngram_value - 1)\n\n    # Combine the tokens\n    return start_tokens + tokens + end_tokens","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:02.637846Z","iopub.execute_input":"2023-12-04T20:04:02.638136Z","iopub.status.idle":"2023-12-04T20:04:02.644337Z","shell.execute_reply.started":"2023-12-04T20:04:02.638111Z","shell.execute_reply":"2023-12-04T20:04:02.643485Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Tokenize data - Execution\n\ndf['title_tokenized'] = df['title'].apply(lambda x: tokenize_and_add_tokens(x, NGRAM))\ndf['selftext_tokenized'] = df['selftext'].apply(lambda x: tokenize_and_add_tokens(x, NGRAM))\n\ndf","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:02.645383Z","iopub.execute_input":"2023-12-04T20:04:02.645624Z","iopub.status.idle":"2023-12-04T20:04:42.361683Z","shell.execute_reply.started":"2023-12-04T20:04:02.645602Z","shell.execute_reply":"2023-12-04T20:04:42.360821Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                   title  \\\n0      \"do not expose any part of your body to the air.\"   \n1      i sometimes remember the way he looked, broken...   \n2      i live alone on the third floor of my apartmen...   \n3      i heard the rain hitting my window, so i walke...   \n4      you know how sometimes your brain plays tricks...   \n...                                                  ...   \n94081  as i look thru at window i see something inhumane   \n94082  i’ve always been passionate about conspiracy t...   \n94083  \"you'll see me on the red carpet one day,\" sai...   \n94084  i could hear my sister screaming nearby as i s...   \n94085  when my neighbor’s girlfriend asked for my hel...   \n\n                                                selftext  score  num_comments  \\\n0                      \"i repeat..this is not a drill..\"   65.0           5.0   \n1                  i neglected to make sure he was dead.   22.0           0.0   \n2      so who opens my window every night while i'm s...   35.0           3.0   \n3      my window wasn't wet, but the glass was covere...   28.0           3.0   \n4                    i caught one of those things today.   84.0           6.0   \n...                                                  ...    ...           ...   \n94081  my reflection helps me remember how well my su...   31.0           2.0   \n94082  so when my wife had twins, i knew exactly what...   27.0           8.0   \n94083  so i paid her a surprise visit, and upon walki...   23.0           2.0   \n94084  but my heart sank when i remembered the monste...   60.0           3.0   \n94085  later when the police found parts of my neighb...   33.0           2.0   \n\n       gilded_count          date            timestamp  \\\n0                 0  1.428090e+09  2015-04-03 19:47:13   \n1                 0  1.428235e+09  2015-04-05 11:55:10   \n2                 0  1.428370e+09  2015-04-07 01:24:42   \n3                 0  1.428385e+09  2015-04-07 05:40:55   \n4                 0  1.428563e+09  2015-04-09 07:03:16   \n...             ...           ...                  ...   \n94081             0  1.680377e+09  2023-04-01 19:21:54   \n94082             0  1.680377e+09  2023-04-01 19:24:55   \n94083             0  1.680378e+09  2023-04-01 19:38:03   \n94084             0  1.680378e+09  2023-04-01 19:41:01   \n94085             0  1.680379e+09  2023-04-01 19:54:23   \n\n                                         title_tokenized  \\\n0      [<s>, <s>, ``, do, not, expose, any, part, of,...   \n1      [<s>, <s>, i, sometimes, remember, the, way, h...   \n2      [<s>, <s>, i, live, alone, on, the, third, flo...   \n3      [<s>, <s>, i, heard, the, rain, hitting, my, w...   \n4      [<s>, <s>, you, know, how, sometimes, your, br...   \n...                                                  ...   \n94081  [<s>, <s>, as, i, look, thru, at, window, i, s...   \n94082  [<s>, <s>, i, ’, ve, always, been, passionate,...   \n94083  [<s>, <s>, ``, you, 'll, see, me, on, the, red...   \n94084  [<s>, <s>, i, could, hear, my, sister, screami...   \n94085  [<s>, <s>, when, my, neighbor, ’, s, girlfrien...   \n\n                                      selftext_tokenized  \n0      [<s>, <s>, ``, i, repeat..this, is, not, a, dr...  \n1      [<s>, <s>, i, neglected, to, make, sure, he, w...  \n2      [<s>, <s>, so, who, opens, my, window, every, ...  \n3      [<s>, <s>, my, window, was, n't, wet, ,, but, ...  \n4      [<s>, <s>, i, caught, one, of, those, things, ...  \n...                                                  ...  \n94081  [<s>, <s>, my, reflection, helps, me, remember...  \n94082  [<s>, <s>, so, when, my, wife, had, twins, ,, ...  \n94083  [<s>, <s>, so, i, paid, her, a, surprise, visi...  \n94084  [<s>, <s>, but, my, heart, sank, when, i, reme...  \n94085  [<s>, <s>, later, when, the, police, found, pa...  \n\n[94086 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>selftext</th>\n      <th>score</th>\n      <th>num_comments</th>\n      <th>gilded_count</th>\n      <th>date</th>\n      <th>timestamp</th>\n      <th>title_tokenized</th>\n      <th>selftext_tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"do not expose any part of your body to the air.\"</td>\n      <td>\"i repeat..this is not a drill..\"</td>\n      <td>65.0</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>1.428090e+09</td>\n      <td>2015-04-03 19:47:13</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, ``, do, not, expose, any, part, of,...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, ``, i, repeat..this, is, not, a, dr...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i sometimes remember the way he looked, broken...</td>\n      <td>i neglected to make sure he was dead.</td>\n      <td>22.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1.428235e+09</td>\n      <td>2015-04-05 11:55:10</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, i, sometimes, remember, the, way, h...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, i, neglected, to, make, sure, he, w...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i live alone on the third floor of my apartmen...</td>\n      <td>so who opens my window every night while i'm s...</td>\n      <td>35.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>1.428370e+09</td>\n      <td>2015-04-07 01:24:42</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, i, live, alone, on, the, third, flo...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, so, who, opens, my, window, every, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i heard the rain hitting my window, so i walke...</td>\n      <td>my window wasn't wet, but the glass was covere...</td>\n      <td>28.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>1.428385e+09</td>\n      <td>2015-04-07 05:40:55</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, i, heard, the, rain, hitting, my, w...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, my, window, was, n't, wet, ,, but, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>you know how sometimes your brain plays tricks...</td>\n      <td>i caught one of those things today.</td>\n      <td>84.0</td>\n      <td>6.0</td>\n      <td>0</td>\n      <td>1.428563e+09</td>\n      <td>2015-04-09 07:03:16</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, you, know, how, sometimes, your, br...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, i, caught, one, of, those, things, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>94081</th>\n      <td>as i look thru at window i see something inhumane</td>\n      <td>my reflection helps me remember how well my su...</td>\n      <td>31.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.680377e+09</td>\n      <td>2023-04-01 19:21:54</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, as, i, look, thru, at, window, i, s...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, my, reflection, helps, me, remember...</td>\n    </tr>\n    <tr>\n      <th>94082</th>\n      <td>i’ve always been passionate about conspiracy t...</td>\n      <td>so when my wife had twins, i knew exactly what...</td>\n      <td>27.0</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>1.680377e+09</td>\n      <td>2023-04-01 19:24:55</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, i, ’, ve, always, been, passionate,...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, so, when, my, wife, had, twins, ,, ...</td>\n    </tr>\n    <tr>\n      <th>94083</th>\n      <td>\"you'll see me on the red carpet one day,\" sai...</td>\n      <td>so i paid her a surprise visit, and upon walki...</td>\n      <td>23.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.680378e+09</td>\n      <td>2023-04-01 19:38:03</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, ``, you, 'll, see, me, on, the, red...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, so, i, paid, her, a, surprise, visi...</td>\n    </tr>\n    <tr>\n      <th>94084</th>\n      <td>i could hear my sister screaming nearby as i s...</td>\n      <td>but my heart sank when i remembered the monste...</td>\n      <td>60.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>1.680378e+09</td>\n      <td>2023-04-01 19:41:01</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, i, could, hear, my, sister, screami...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, but, my, heart, sank, when, i, reme...</td>\n    </tr>\n    <tr>\n      <th>94085</th>\n      <td>when my neighbor’s girlfriend asked for my hel...</td>\n      <td>later when the police found parts of my neighb...</td>\n      <td>33.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.680379e+09</td>\n      <td>2023-04-01 19:54:23</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, when, my, neighbor, ’, s, girlfrien...</td>\n      <td>[&lt;s&gt;, &lt;s&gt;, later, when, the, police, found, pa...</td>\n    </tr>\n  </tbody>\n</table>\n<p>94086 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize data - Ref Variable\n\ntokenized_data = pd.concat([df['title_tokenized'], df['selftext_tokenized']]).to_list()\ntokenized_data_X = df['title_tokenized'].to_list()\ntokenized_data_Y = df['selftext_tokenized'].to_list()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:42.362996Z","iopub.execute_input":"2023-12-04T20:04:42.363304Z","iopub.status.idle":"2023-12-04T20:04:42.384450Z","shell.execute_reply.started":"2023-12-04T20:04:42.363278Z","shell.execute_reply":"2023-12-04T20:04:42.383589Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Tokenize Data - Create Dictionary Mapping\n\ntokenizer_on_words = Tokenizer()\n# Update tokenizer vocab on data\ntokenizer_on_words.fit_on_texts(tokenized_data)\n# Convert list of texts to list of integers\nencoded_on_words = tokenizer_on_words.texts_to_sequences(tokenized_data)\n\n# tokenizer_on_words.word_index\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:42.385779Z","iopub.execute_input":"2023-12-04T20:04:42.386394Z","iopub.status.idle":"2023-12-04T20:04:48.181240Z","shell.execute_reply.started":"2023-12-04T20:04:42.386360Z","shell.execute_reply":"2023-12-04T20:04:48.180235Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Print out the size of the vocab\n\nsize_of_vocab = len(tokenizer_on_words.word_index)\nprint(\"Size of Word Index (on words):\", size_of_vocab)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:48.182530Z","iopub.execute_input":"2023-12-04T20:04:48.182869Z","iopub.status.idle":"2023-12-04T20:04:48.187965Z","shell.execute_reply.started":"2023-12-04T20:04:48.182842Z","shell.execute_reply":"2023-12-04T20:04:48.186994Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Size of Word Index (on words): 45626\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3. Prepare Sequences to Train Model\n#### Fixed n-gram based sequences","metadata":{}},{"cell_type":"code","source":"def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n    '''\n    Takes the encoded data (list of lists) and\n    generates the training samples out of it.\n\n    Parameters:\n        encoded (list): list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n        ngram (int): the ngram model you are training\n\n    Returns:\n        training_samples (list): list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n    '''\n    training_samples = []\n\n    for sentence in encoded:\n        for i in range(len(sentence) - ngram + 1):\n            training_samples.append(sentence[i:i+ngram])\n\n    return training_samples\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:48.189495Z","iopub.execute_input":"2023-12-04T20:04:48.189884Z","iopub.status.idle":"2023-12-04T20:04:48.197999Z","shell.execute_reply.started":"2023-12-04T20:04:48.189852Z","shell.execute_reply":"2023-12-04T20:04:48.197168Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"word_training_samples = generate_ngram_training_samples(encoded=encoded_on_words, ngram=NGRAM)\n\nprint(\"Word Training Samples (first 5):\")\nprint(word_training_samples[:5])\n\nprint(\"\\nNum Word Training Samples:\", len(word_training_samples))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:48.201421Z","iopub.execute_input":"2023-12-04T20:04:48.201703Z","iopub.status.idle":"2023-12-04T20:04:53.198015Z","shell.execute_reply.started":"2023-12-04T20:04:48.201680Z","shell.execute_reply":"2023-12-04T20:04:53.197029Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Word Training Samples (first 5):\n[[1, 1, 21], [1, 21, 61], [21, 61, 58], [61, 58, 8594], [58, 8594, 266]]\n\nNum Word Training Samples: 3802291\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4. Create Word Embedding Vectors based on dataset","metadata":{}},{"cell_type":"code","source":"# Use gensim.Word2Vec to create embeddings\npath = False\n# path = 'two_sentence_horror_reddit_word_embeddings.txt'\n\nif not path:\n    word_embeddings: Word2Vec = Word2Vec(tokenized_data, min_count=1, vector_size=EMBEDDINGS_SIZE, window=5, sg=1)\nelse:\n    word_embeddings: Word2Vec = Word2Vec.load(path)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:04:53.199114Z","iopub.execute_input":"2023-12-04T20:04:53.199396Z","iopub.status.idle":"2023-12-04T20:05:32.001235Z","shell.execute_reply.started":"2023-12-04T20:04:53.199371Z","shell.execute_reply":"2023-12-04T20:05:31.999942Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Save the embeddings\nword_embeddings.wv.save_word2vec_format('two_sentence_horror_reddit_word_embeddings.txt', binary=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:32.002523Z","iopub.execute_input":"2023-12-04T20:05:32.002949Z","iopub.status.idle":"2023-12-04T20:05:35.332987Z","shell.execute_reply.started":"2023-12-04T20:05:32.002906Z","shell.execute_reply":"2023-12-04T20:05:35.331972Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 5. Split Sequences into X and y, then create data generator","metadata":{}},{"cell_type":"code","source":"# Note here that the sequences were in the form:\n# sequence = [x1, x2, ... , x(n-1), y]\n# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n\nseq_on_words_x = [seq[:-1] for seq in word_training_samples]\nseq_on_words_y = [seq[-1] for seq in word_training_samples]\n\n# Print out the shapes to verify that they are correct\nprint(\"Shape of Word X:\", len(seq_on_words_x))\nprint(\"Shape of Word Y:\", len(seq_on_words_y))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:35.334144Z","iopub.execute_input":"2023-12-04T20:05:35.334421Z","iopub.status.idle":"2023-12-04T20:05:38.839870Z","shell.execute_reply.started":"2023-12-04T20:05:35.334399Z","shell.execute_reply":"2023-12-04T20:05:38.838925Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Shape of Word X: 3802291\nShape of Word Y: 3802291\n","output_type":"stream"}]},{"cell_type":"code","source":"def read_embeddings(embeddings: Word2Vec | str, tokenizer: Tokenizer) -> (dict, dict):\n    '''\n    Loads and parses embeddings trained in earlier.\n\n    Parameters:\n        embeddings (Word2Vec): embeddings trained on the data\n        Tokenizer (Tokenizer): tokenizer used to tokenize the data (needed to get the word to index mapping)\n        is_char (bool): determines if embeddings is character or word based\n\n    Returns:\n        word_to_embedding (dict): mapping from word to its embedding vector\n        index_to_embedding (dict): mapping from index to its embedding vector\n    '''\n    # Retrieve dict of word to index from tokenizer\n    if path:\n        embeddings = KeyedVectors.load_word2vec_format(embeddings, binary=False)\n    tokenizer_words = list(tokenizer.word_index.keys())\n    tokenizer_words_to_idx = tokenizer.word_index\n\n    # Create a dict of word to embedding\n    word_to_embedding = {word: embeddings.wv[word] for word in tokenizer_words}\n    index_to_embedding = {tokenizer_words_to_idx[word]: embeddings.wv[word] for word in tokenizer_words}\n\n    # Add zero vector for padding token\n    # embedding_size = embeddings.wv.vector_size\n    word_to_embedding['<PAD>'] = np.zeros(EMBEDDINGS_SIZE)\n    index_to_embedding[0] = np.zeros(EMBEDDINGS_SIZE)\n\n    return word_to_embedding, index_to_embedding\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:38.841251Z","iopub.execute_input":"2023-12-04T20:05:38.841953Z","iopub.status.idle":"2023-12-04T20:05:38.849162Z","shell.execute_reply.started":"2023-12-04T20:05:38.841915Z","shell.execute_reply":"2023-12-04T20:05:38.848186Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"word_to_embedding_on_word, index_to_embedding_on_word = read_embeddings(word_embeddings, tokenizer_on_words)\n\nprint(len(word_to_embedding_on_word))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:38.850307Z","iopub.execute_input":"2023-12-04T20:05:38.850580Z","iopub.status.idle":"2023-12-04T20:05:39.062181Z","shell.execute_reply.started":"2023-12-04T20:05:38.850549Z","shell.execute_reply":"2023-12-04T20:05:39.061272Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"45627\n","output_type":"stream"}]},{"cell_type":"code","source":"# the \"0\" index of the Tokenizer is assigned for the padding token. \n# We init the vector for padding token as all zeros of embedding size\n\n# Modify tokenizer to include padding token\npadding_token = '<PAD>'\ntokenizer_on_words.word_index[padding_token] = 0\ntokenizer_on_words.index_word[0] = padding_token\n\n# Increase vocab size\nsize_of_vocab += 1\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:39.063420Z","iopub.execute_input":"2023-12-04T20:05:39.063778Z","iopub.status.idle":"2023-12-04T20:05:39.070273Z","shell.execute_reply.started":"2023-12-04T20:05:39.063729Z","shell.execute_reply":"2023-12-04T20:05:39.069464Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"tokenizer_json = tokenizer_on_words.to_json()\nwith io.open('tokenizer_keras.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:39.071319Z","iopub.execute_input":"2023-12-04T20:05:39.071625Z","iopub.status.idle":"2023-12-04T20:05:39.302361Z","shell.execute_reply.started":"2023-12-04T20:05:39.071595Z","shell.execute_reply":"2023-12-04T20:05:39.301535Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (list,list):\n    '''\n    Returns data generator to be used by feed_forward\n    https://wiki.python.org/moin/Generators\n    https://realpython.com/introduction-to-python-generators/\n\n    Yields batches of embeddings and labels to go with them.\n    Use one hot vectors to encode the labels\n    (see the to_categorical function)\n\n    Parameters:\n        X (list): list of lists in the format [[x1, x2, ... , x(n-1)], ...]\n        y (list): list of labels in the format [y1, y2, ...]\n        num_sequences_per_batch (int): number of sequences per batch\n        index_2_embedding (dict): mapping from index to its embedding vector\n        is_char (bool): determines if embeddings is character or word based\n\n    Returns:\n        X_batch_embeddings (np.ndarray): list of embeddings in the format [[x1, x2, ... , x(n-1)], ...]\n        y_batch_onehot (np.ndarray): list of one hot vectors in the format [y1, y2, ...]\n    '''\n    num_samples = len(y) # num sequences (X and y should be same length)\n    num_classes = len(index_2_embedding) # num classes = size of vocab\n\n    while True: # restart generator when it reaches the end for Keras\n        for start_index in range(0, num_samples, num_sequences_per_batch): # by batch\n            end_index = min(start_index + num_sequences_per_batch, num_samples) # if last batch goes over total num of samples\n\n            # Get batch of sequences and labels\n            X_batch_sequences = X[start_index:end_index]\n            y_batch_labels = y[start_index:end_index]\n\n            # Convert sequences into embeddings and flatten them\n            X_batch_embeddings = []\n            for seq in X_batch_sequences:\n                X_batch_embeddings.append(np.array([index_2_embedding[token] for token in seq]).flatten())\n\n            # Convert labels to one-hot encoded vectors\n            y_batch_onehot = to_categorical(y_batch_labels, num_classes=num_classes)\n\n            yield np.array(X_batch_embeddings), np.array(y_batch_onehot)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:39.303503Z","iopub.execute_input":"2023-12-04T20:05:39.303856Z","iopub.status.idle":"2023-12-04T20:05:39.312571Z","shell.execute_reply.started":"2023-12-04T20:05:39.303830Z","shell.execute_reply":"2023-12-04T20:05:39.311719Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Init data generator\n\nnum_sequences_per_batch = 128 # this is the batch size\nsteps_per_epoch = len(seq_on_words_x) // num_sequences_per_batch  # Number of batches per epoch\ntrain_generator_on_word = data_generator(seq_on_words_x, seq_on_words_y, num_sequences_per_batch, index_to_embedding_on_word)\n\nsample = next(train_generator_on_word) # this is how you get data out of generators\nprint(\"Word X Shape:\", sample[0].shape) # (batch_size, (n-1) * EMBEDDING_SIZE)  (128, 100)\nprint(\"Word Y Shape:\", sample[1].shape) # (batch_size, |V|) to_categorical\n\n\n# TODO\n# Dimensions:\n\n# Word X Shape: (128, 200)\n# Word Y Shape: (128, 45627) (vocab + 1 for padding)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:39.313710Z","iopub.execute_input":"2023-12-04T20:05:39.314063Z","iopub.status.idle":"2023-12-04T20:05:39.341588Z","shell.execute_reply.started":"2023-12-04T20:05:39.314020Z","shell.execute_reply":"2023-12-04T20:05:39.340855Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Word X Shape: (128, 200)\nWord Y Shape: (128, 45627)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 6. Train and Save the Model","metadata":{}},{"cell_type":"code","source":"def create_feedforward_neural_lm(input_shape: int, vocab_size: int) -> Sequential:\n    \"\"\"\n    Create a Feedforward Neural Language Model.\n\n    Args:\n        input_shape (int): shape of input data ((n-1) * EMBEDDING_SIZE))\n        vocab_size (int): size of vocab\n\n    Returns:\n        model (Sequential): Keras Sequential model\n    \"\"\"\n    # Init model\n    model = Sequential()\n\n    # Define hidden layer(s)\n    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))\n    model.add(Dropout(0.5))\n\n    # Output layer\n    model.add(Dense(vocab_size, activation='softmax'))\n\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:39.342890Z","iopub.execute_input":"2023-12-04T20:05:39.343161Z","iopub.status.idle":"2023-12-04T20:05:39.348624Z","shell.execute_reply.started":"2023-12-04T20:05:39.343136Z","shell.execute_reply":"2023-12-04T20:05:39.347770Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Callbacks\ncheckpoint = ModelCheckpoint('basic_keras_model.h5', save_best_only=True, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5)\ntensorboard = TensorBoard(log_dir='./logs')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:39.349956Z","iopub.execute_input":"2023-12-04T20:05:39.350221Z","iopub.status.idle":"2023-12-04T20:05:39.361139Z","shell.execute_reply.started":"2023-12-04T20:05:39.350195Z","shell.execute_reply":"2023-12-04T20:05:39.360388Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Train Word Model\n\ntrain_generator_on_word = data_generator(seq_on_words_x, seq_on_words_y, num_sequences_per_batch, index_to_embedding_on_word)\n\nword_model = create_feedforward_neural_lm(input_shape=(NGRAM-1) * EMBEDDINGS_SIZE, vocab_size=size_of_vocab)\nword_model.fit(x=train_generator_on_word,\n            steps_per_epoch=len(seq_on_words_x) // num_sequences_per_batch,\n            epochs=1,\n            callbacks=[checkpoint, early_stopping, tensorboard])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:05:39.362056Z","iopub.execute_input":"2023-12-04T20:05:39.362377Z","iopub.status.idle":"2023-12-04T20:34:14.681643Z","shell.execute_reply.started":"2023-12-04T20:05:39.362346Z","shell.execute_reply":"2023-12-04T20:34:14.680635Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"29705/29705 [==============================] - 1712s 58ms/step - loss: 5.0584 - accuracy: 0.2304\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x78960ce8b970>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate Model\n\naccuracy_word = word_model.evaluate(x=train_generator_on_word, steps=len(seq_on_words_x) // num_sequences_per_batch)\n\nprint(\"Word Model - Accuracy:\", accuracy_word)\n\n# TODO:\n# Accuracy:\n# 20.37% for word model","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:34:14.683033Z","iopub.execute_input":"2023-12-04T20:34:14.683343Z","iopub.status.idle":"2023-12-04T21:02:36.838876Z","shell.execute_reply.started":"2023-12-04T20:34:14.683315Z","shell.execute_reply":"2023-12-04T21:02:36.837809Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"29705/29705 [==============================] - 1678s 56ms/step - loss: 4.7697 - accuracy: 0.2416\nWord Model - Accuracy: [4.769742965698242, 0.24160774052143097]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save Model\n\nword_model.save('two_sentence_horror_seq_keras_1_epoch.keras')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T21:02:36.840338Z","iopub.execute_input":"2023-12-04T21:02:36.840725Z","iopub.status.idle":"2023-12-04T21:02:37.111437Z","shell.execute_reply.started":"2023-12-04T21:02:36.840690Z","shell.execute_reply":"2023-12-04T21:02:37.110421Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Print Model Summaries\n\nprint(\"Word Model Summary:\")\nword_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T21:02:37.112987Z","iopub.execute_input":"2023-12-04T21:02:37.113289Z","iopub.status.idle":"2023-12-04T21:02:37.130621Z","shell.execute_reply.started":"2023-12-04T21:02:37.113264Z","shell.execute_reply":"2023-12-04T21:02:37.129769Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Word Model Summary:\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 128)               25728     \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 45627)             5885883   \n                                                                 \n=================================================================\nTotal params: 5911611 (22.55 MB)\nTrainable params: 5911611 (22.55 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 7. Generate Sentences","metadata":{}},{"cell_type":"code","source":"def generate_seq(model: Sequential, tokenizer: Tokenizer, seed: str, max_length: int):\n    '''\n    Generate a sequence from the model until you get an end of sentence token.\n\n    Parameters:\n        model: your neural network\n        tokenizer: the keras preprocessing tokenizer\n        seed: a string that serves as the starting point of the sequence\n        max_length: maximum amount of tokens to generate\n\n    Returns: string sentence\n    '''\n    # Convert string seed into a list of words\n    sentence = seed.split()\n    \n    # Rest of your code remains the same\n    eos_idx = tokenizer.word_index['</s>']\n    bos_idx = tokenizer.word_index['<s>']\n    itr = 0\n    first_sentence_done = False\n\n    while itr < max_length:\n        # Convert given input into indices, then embeddings\n        input_tokens = sentence[-(NGRAM-1):]\n        encoded_indices = tokenizer.texts_to_sequences([input_tokens])[0]\n\n        # Pad seq if length < NGRAM-1 (for last token)\n        while len(encoded_indices) < NGRAM-1:\n            encoded_indices.insert(0, bos_idx)  # prepend with zeros or a specific padding token index\n\n        # Convert indices to embeddings, then flatten into 1D array, then reshape to 2D array to fit model\n        # Model expects: 2D array of shape (1, (n-1) * EMBEDDINGS_SIZE)\n        encoded_embeddings = np.array([index_to_embedding_on_word[idx] for idx in encoded_indices]).flatten().reshape(1, -1)\n\n        # Predict next token\n        prediction = model.predict(encoded_embeddings)\n        predicted_idx = np.random.choice(len(prediction[0]), p=prediction[0]) # generate random sentence\n        predicted_word = tokenizer.index_word[predicted_idx]\n\n        # If next token is end of sentence, return sentence\n        if predicted_word == '</s>':\n            first_sentence_done = True\n            if first_sentence_done:\n                return ' '.join(sentence)\n        else:\n            sentence.append(predicted_word)\n\n        itr += 1\n\n    return ' '.join(sentence)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T22:19:31.122888Z","iopub.execute_input":"2023-12-04T22:19:31.123851Z","iopub.status.idle":"2023-12-04T22:19:31.133096Z","shell.execute_reply.started":"2023-12-04T22:19:31.123818Z","shell.execute_reply":"2023-12-04T22:19:31.132358Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Generate Text - Interactive\n\ninput_sentences = ['I got out of bed this morning.', \n                   'I was horrified when I get my test results back.',\n                   'My parents told me not to go upstairs.',\n                   'There was a ghost.']\ngenerated_stories = []\nmax_length = 99999 # \"no max length\" - generate until </s> token\n\nfor input_seq in input_sentences:\n    word_sentence = generate_seq(word_model, tokenizer_on_words, input_seq, max_length)\n    generated_stories.append(word_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T22:21:00.504620Z","iopub.execute_input":"2023-12-04T22:21:00.505483Z","iopub.status.idle":"2023-12-04T22:21:04.589452Z","shell.execute_reply.started":"2023-12-04T22:21:00.505447Z","shell.execute_reply":"2023-12-04T22:21:04.588630Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 16ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"for gen_sentence in generated_stories:\n    print(gen_sentence)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T22:21:29.838512Z","iopub.execute_input":"2023-12-04T22:21:29.839258Z","iopub.status.idle":"2023-12-04T22:21:29.844123Z","shell.execute_reply.started":"2023-12-04T22:21:29.839227Z","shell.execute_reply":"2023-12-04T22:21:29.843279Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"I got out of bed this morning. look on him tied by the plan , they were those “ didn ’ t notice to their medicine , i can finally make me more my words now standing on his alarm father .\n\n\nI was horrified when I get my test results back. , they were starting to harvest , having stopped walking here .\n\n\nMy parents told me not to go upstairs. through screams and suffering , was that when will speak to worry .\n\n\nThere was a ghost. before ten , as he who would send the power is true dead .\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
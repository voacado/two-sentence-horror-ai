{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your dataset\n",
    "# df = pd.read_csv('../../dataset/reddit_cleansed_data.csv')\n",
    "\n",
    "# # Calculate weighted score (how good a post is)\n",
    "# df['weighted_score'] = df['score'] + (10 * df['num_comments']) + (100 * df['gilded_count']) \n",
    "\n",
    "# # view all posts where score over 500\n",
    "# df\n",
    "# # df[df['weighted_score'] > 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Evaluation Metric (BERTScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Compute and Save Embeddings for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    \"\"\"Generate BERT embeddings for the given text\"\"\"\n",
    "    with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "        tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "        outputs = model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu().numpy()  # Move to CPU and convert to numpy\n",
    "\n",
    "# # Compute embeddings and collect scores\n",
    "# embeddings = []\n",
    "# scores = []\n",
    "# for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Computing embeddings\"):\n",
    "#     story = row['title'] + ' ' + row['selftext']\n",
    "#     embedding = embed_text(story)\n",
    "#     embeddings.append(embedding[0])  # embedding is a 1x768 numpy array; we take the first element\n",
    "#     scores.append(row['weighted_score'])\n",
    "\n",
    "# # Combine embeddings and scores into a DataFrame\n",
    "# embeddings_df = pd.DataFrame({\n",
    "#     'embedding': list(embeddings),\n",
    "#     'weighted_score': scores\n",
    "# })\n",
    "\n",
    "# # Save the DataFrame as a Pickle file\n",
    "# embeddings_df.to_pickle('story_embeddings_with_scores.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalulate Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.15318204, 0.017788239, 0.23121382, 0.050800...</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.038681276, 0.13021308, 0.1265131, -0.060362...</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           embedding  weighted_score\n",
       "0  [0.15318204, 0.017788239, 0.23121382, 0.050800...           115.0\n",
       "1  [0.038681276, 0.13021308, 0.1265131, -0.060362...            22.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DataFrame\n",
    "embeddings_df = pd.read_pickle('story_embeddings_with_scores.pkl')\n",
    "embeddings_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_story(new_story):\n",
    "#     # Embed the new story\n",
    "#     new_story_embedding = embed_text(new_story)[0]  # Get the first element of the embedding\n",
    "\n",
    "#     # Calculate similarities and weighted scores\n",
    "#     similarities = []\n",
    "#     for index, row in embeddings_df.iterrows():\n",
    "#         story_embedding = row['embedding']\n",
    "#         similarity = cosine_similarity([new_story_embedding], [story_embedding])[0, 0]\n",
    "#         weighted_score = row['weighted_score']\n",
    "#         similarities.append((similarity, weighted_score))\n",
    "\n",
    "#     # Calculate a final score for the new story\n",
    "#     # This can be a simple average of the weighted scores weighted by similarity\n",
    "#     if similarities:\n",
    "#         total_weighted_score = sum(similarity * score for similarity, score in similarities)\n",
    "#         total_similarity = sum(similarity for similarity, _ in similarities)\n",
    "#         final_score = total_weighted_score / total_similarity if total_similarity else 0\n",
    "#     else:\n",
    "#         final_score = 0\n",
    "\n",
    "#     return final_score\n",
    "\n",
    "\n",
    "\n",
    "def eval_story(new_story):\n",
    "    # Embed the new story\n",
    "    new_story_embedding = embed_text(new_story)[0]  # Get the first element of the embedding\n",
    "\n",
    "    # Initialize variables to track the most similar story\n",
    "    max_similarity = -1  # Start with a similarity that's lower than the lowest possible\n",
    "    most_similar_score = 0\n",
    "\n",
    "    # Iterate through each story in the dataset\n",
    "    for index, row in embeddings_df.iterrows():\n",
    "        story_embedding = row['embedding']\n",
    "        similarity = cosine_similarity([new_story_embedding], [story_embedding])[0, 0]\n",
    "        \n",
    "        # Check if this story is more similar than the current most similar\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_score = row['weighted_score']\n",
    "\n",
    "    return most_similar_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.0\n"
     ]
    }
   ],
   "source": [
    "# Evalulate story\n",
    "\n",
    "# story = \"My cat had a botfly in his eye, so I had to pull it out. One hour and three feet later and I'm still pulling.\"\n",
    "# story = \"All my life, my parents have told me not to open the basement door, but I got curious and disobeyed them. What is that glowing ball in the sky and why does it hurt my eyes?\"\n",
    "story = \"I ate hamburger. But I didnâ€™t realize it was hamburger.\"\n",
    "print(eval_story(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
